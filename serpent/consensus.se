CATCH_TOLERANCE = 0.1
CATCH_HI = 0.5 * (1 - CATCH_TOLERANCE)
CATCH_LO = 0.5 * (1 + CATCH_TOLERANCE) 
HALF = 0.5
ALPHA = 0.1
MAX_ROW = 5000
TOTAL_REP = 20
COINS = 10^6

data reputation[]
data votes[]
data num_events
data num_voters

def init():
    self.votes = [[1, 1, 0, 0], [1, 0, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0], [0, 0, 1, 1], [0, 0, 1, 1]]
    self.num_events = 4
    self.num_voters = 6
    self.reputation = [2, 10, 4, 2, 7, 1]
    self.total_rep = 24
    self.rep_coins = self.reputation * COINS

# Arithmetic mean of an array
macro mean($a:$asz):
    with total = 0:
        with i = 0:
            while i < $asz:
                total += $a[i]
                i += 1
        total * 0x10000000000000000 / i

# Dot (inner) product of vectors.
macro dot($a, $b, $len):
    with i = 0:
        with prod = 0:
            while i < $len:
                prod += $a[i] * $b[i]
                i += 1
            prod

# Sum elements of array
macro sum($a:$asz):
    with total = 0:
        with i = 0:
            while i < $asz:
                total += $a[i]
                i += 1
        total

# Normalize array (elements sum to 1)
macro normalize($a:$asz):
    with anorm = array($asz):
        with total = 0:
            with i = 0:
                while i < $asz:
                    if $a[i] != None:
                        total += $a[i]
                    i += 1
            with i = 0:
                while i < $asz:
                    if $a[i] != None:
                        anorm[i] = $a[i] * 0x10000000000000000 / total
                    i += 1
            anorm

# Calculates the outer product of vectors.
macro outer_product($u:$usz, $v:$vsz):
    with p = array($usz):
        with i = 0:
            while i < $usz:
                p[i] = array($usz)
                with j = 0:
                    while j < $usz:
                        p[i][j] += $u[i] * $v[j]
                        j += 1
                i += 1
            p

# Vector Kronecker product
macro kron($a:$asz, $b:$bsz):
    with prod = array($asz * $bsz):
        with i = 0:
            while i < $asz:
                with j = 0:
                    while j < $bsz:
                        prod[j + $bsz*i] = $a[i] * $b[j]
                        j += 1
                i += 1
        prod

# Matrix multiplication, inputs are flattened (vectorized) matrices
macro multiply($aflat:$asz, $arows, $acols, $bflat:$bsz, $brows, $bcols):
    with a = array($arows):
        with i = 0:
            while i < $arows:
                a[i] = array($acols)
                with j = 0:
                    while j < $acols:
                        a[i][j] = $aflat[j + i*$acols]
                        j += 1
                i += 1
        with b = array($brows):
            with i = 0:
                while i < $brows:
                    b[i] = array($bcols)
                    with j = 0:
                        while j < $bcols:
                            b[i][j] = $bflat[j + i*$bcols]
                            j += 1
                    i += 1
        with c = array($arows):
            if $bcols > 1:
                with i = 0:
                    while i < $arows:
                        c[i] = array($bcols)
                        i += 1
            with i = 0:
                while i < $arows:
                    with j = 0:
                        while j < $bcols:
                            with k = 0:
                                while k < $acols:
                                    if $bcols == 1:
                                        c[i] += a[i][k] * b[k]
                                    else:
                                        c[i][j] += a[i][k] * b[k][j]
                                    k += 1
                            j += 1
                    i += 1
                c

# Swap the rows and columns of a matrix
macro transpose($aflat:$asz, $arows, $acols):
    with a = array($arows):
        with i = 0:
            while i < $arows:
                a[i] = array($acols)
                with j = 0:
                    while j < $acols:
                        a[i][j] = $aflat[j + i*$acols]
                        j += 1
                i += 1
        with at = array($acols):
            with i = 0:
                while i < $asz:
                    at[i] = array($arows)
                    i += 1
            with i = 0:
                while i < $acols:
                    with j = 0:
                        while j < $arows:
                            at[i][j] = a[j][i]
                            j += 1
                    i += 1
            at

# Convert vector to diagonal matrix
macro diag($a:$asz):
    with d = array($asz):
        with i = 0:
            while i < $asz:
                d[i] = array($asz)
                with j = 0:
                    while j < $asz:
                        if i == j:
                            d[i][j] = $a[i]
                        else:
                            d[i][j] = 0
                        j += 1
                i += 1
        d

macro isnan($a:$asz):
    with amask = array($asz):
        with i = 0:
            while i < $asz:
                if $a[i] == None:
                    amask[i] = 1
                else:
                    amask[i] = 0
                i += 1
            amask

macro mask($a:$asz, $target):
    with amask = array($asz):
        with i = 0:
            while i < $asz:
                if $a[i] == $target:
                    amask[i] = 1
                else:
                    amask[i] = 0
                i += 1
            amask

macro any($a:$asz):
    with result = 0:
        with i = 0: 
            while i < $asz:
                if $a[i] != 0:
                    result = 1
                    break
                i += 1
            result

# Hadamard (elementwise) product, inputs are flattened (vectorized) matrices
macro hadamard($aflat:$asz, $arows, $acols, $bflat:$bsz, $brows, $bcols):
    with a = array($arows):
        with i = 0:
            while i < $arows:
                a[i] = array($acols)
                with j = 0:
                    while j < $acols:
                        a[i][j] = $aflat[j + i*$acols]
                        j += 1
                i += 1
        with b = array($brows):
            with i = 0:
                while i < $brows:
                    b[i] = array($bcols)
                    with j = 0:
                        while j < $bcols:
                            b[i][j] = $bflat[j + i*$bcols]
                            j += 1
                    i += 1
            with c = array($arows):
                if $bcols > 1:
                    with i = 0:
                        while i < $bcols:
                            c[i] = array($bcols)
                            i += 1
                with i = 0:
                    while i < $arows:
                        if $bcols == 1 and $acols == 1:
                            c[i] = a[i] * b[i]
                        else:
                            with j = 0:
                                while j < $bcols:
                                    c[i][j] += a[i][j] * b[i][j]
                                    j += 1
                        i += 1
                c

# Proportional distances from zero
macro get_weight($a:$asz):
    with total = 0:
        with i = 0:
            while i < $asz:
                total += $a[i]
                i += 1
    with total = 0:
        with i = 0:
            while i < $asz:
                total += $a[i]
                i += 1
        with i = 0:
            with b = array($asz):
                while i < $asz:
                    # only fractional part is needed
                    b[i] = $a[i] * 0x10000000000000000 / total
                i += 1
            b

# Bins values to 0, .5, and 1
macro catch($x):
    with caught = 0:
        if $x < CATCH_LO:
            caught = 0
        elif $x > CATCH_HI:
            caught = 1
        else:
            caught = HALF
        caught

# Linear time deterministic PCA
def emwpca(data:a, weights:a):
    num_obs = arglen(weights)
    num_params = arglen(data) / arglen(weights)

    weighted_means = array(num_params)
    total_weight = 0
    i = 0
    while i < num_obs:
        j = 0
        while j < num_params:
            weighted_means[j] += weights[i] * data[i * num_params + j]
            j += 1
        total_weight += weights[i]
        i += 1

    j = 0
    while j < num_params:
        weighted_means[j] /= total_weight
        j += 1

    weighted_centered_data = array(arglen(data))
    i = 0
    while i < arglen(data):
        weighted_centered_data[i] = data[i] - weighted_means[i % num_params]
        i += 1

    # initialize the loading vector
    loading_vector = array(num_params)
    loading_vector[0] = 0x10000000000000000

    i = 0
    # for j in xrange(ITERATIONS_CAP):
    while i < 25:
        # s = numpy.zeros(num_params)
        s = array(num_params)
        # for wcdatum, weight in zip(weighted_centered_data, weights):
        j = 0
        while j < num_obs:
            # s -= wcdatum.dot(loadings[i]) * wcdatum * weight
            d_dot_lv = 0
            k = 0
            while k < num_params:
                d_dot_lv += weighted_centered_data[j * num_params + k] * loading_vector[k]
                k += 1
            d_dot_lv /= 0x10000000000000000
            k = 0
            while k < num_params:
                s[k] -= d_dot_lv * weighted_centered_data[j * num_params + k] * weights[j]
                k += 1
            j += 1
        # loading_vector = normalize(s)
        # (first rejig s to account for double fixed multiplication in loop)
        j = 0
        while j < num_params:
            s[j] /= 0x100000000000000000000000000000000
            j += 1
        # QQ
        s_dot_s = 0
        j = 0
        while j < num_params:
            s_dot_s += s[j] * s[j]
            j += 1
        s_dot_s /= 0x10000000000000000
        # QQ!!!!
        norm_s = s_dot_s / 2
        j = 0
        while j < 11:
            norm_s = (norm_s + s_dot_s*0x10000000000000000/norm_s) / 2
            j += 1
        # fuggin assign
        j = 0
        while j < num_params:
            loading_vector[j] = s[j]*0x10000000000000000/norm_s
            j += 1

        i += 1

    return(loading_vector, num_params)

def consensus(votes, num_voters, num_events):
    # Use exisiting data and reputations to fill missing observations.
    # Weighted average using all non-missing data.
    v_size = self.num_voters * self.num_events
    votes_flat = array(v_size)
    i = 0
    while i < num_voters:
        j = 0
        while j < num_events:
            votes_flat[j + i*num_events] = self.votes[i][j]
            j += 1
        i += 1
    if any(mask(votes_flat)):
        outcomes_raw = array(num_events)
        i = 0
        while i < num_events:

            # The Reputation of the rows (players) who DID provide
            # judgements, rescaled to sum to 1.
            active_players_rep = self.reputation[-mask(votes[:,i])]

            # Set missing values to 0
            missing_values = isnan(active_players_rep)
            active_players_rep[missing_values] = 0

            # Normalize
            active_players_rep /= array_sum(active_players_rep)

            # The relevant Event with NAs removed.
            # ("What these players had to say about the Events
            # they DID judge?")
            active_events = votes[-mask(votes[:,i]), i]

            # Current best-guess for this Binary Event (weighted average)
            outcomes_raw[i] = multiply(active_events, active_players_rep)
            i += 1

        outcomes_raw = transpose(outcomes_raw)
        na_mat = mask(votes)  # Slice of the matrix which needs to be edited.
        votes_filled[na_mat] = 0  # Erase the NA's
        NAsToFill = multiply(na_mat, diag(outcomes_raw))
        votes_filled += NAsToFill
        i = 0
        while i < num_voters:
            j = 0
            while j < num_events:
                votes_filled[i][j] = catch(votes_filled[i][j])
                j += 1
            i += 1

    # Calculate new reputations using Sztorc consensus.
    scores = self.emwpca(votes_filled:num_voters)

    # Which of the two possible 'new' reputation vectors had more opinion in common
    # with the original 'old' reputation.
    set1 = scores + abs(min(scores))
    set2 = scores - max(scores)
    old = multiply(transpose(self.reputation), votes_filled)
    new1 = multiply(get_weight(set1), votes_filled)
    new2 = multiply(get_weight(set2), votes_filled)

    # Difference in sum of squared errors. If > 0, then new1 had higher
    # errors (use new2); conversely if < 0, then use new1.
    ref_ind = sum((new1 - old)^2) - sum((new2 - old)^2)
    if ref_ind <= 0:
        adj_prin_comp = set1
    if ref_ind > 0:
        adj_prin_comp = set2
  
    # Set this to uniform if you want a passive diffusion toward equality
    # when people cooperate.  Instead diffuses towards previous reputation.
    # (Smoothing does this anyway!)
    #
    # Note: reputation/mean(reputation) is a correction ensuring Reputation is
    # additive. Therefore, nothing can be gained by splitting/combining
    # Reputation into single/multiple accounts.
    row_reward_weighted = self.reputation
    if max(abs(adj_prin_comp)) != 0:
        # Overwrite the inital declaration IFF there wasn't perfect consensus.
        row_reward_weighted = get_weight(adj_prin_comp * transpose(self.reputation / self.mean(self.reputation)))

          
    # Freshly-Calculated Reward (Reputation) - Exponential Smoothing
    # New Reward: row_reward_weighted
    # Old Reward: reputation
    smooth_rep = ALPHA*row_reward_weighted + (1 - ALPHA)*transpose(self.reputation)

    outcomes_raw = multiply(smooth_rep, votes_filled)
    outcome_final = array(num_events)
    i = 0
    while i < num_events:
        outcome_final[i] = catch(outcomes_raw[i])
        i += 1

    # .5 is obviously undesireable, this function travels from 0 to 1
    # with a minimum at .5
    certainty = abs(2 * (outcomes_raw - 0.5))
    # Grading Authors on a curve.
    consensus_reward = get_weight(certainty)
    # How well did beliefs converge?
    avg_certainty = self.mean(certainty)

    # Participation: information about missing values
    na_mat = self.votes * 0
    na_mat_mask = mask(na_mat)
    na_mat[na_mat_mask] = 1  # missing-value indicator matrix
    # Participation within events (columns)
    participation_columns = 1 - multiply(smooth_rep, na_mat)
    # Participation within agents (Rows)
    participation_rows = 1 - na_mat.sum(axis=1) / na_mat.shape[1]
    # General participation
    percent_na = 1 - mean(participation_columns)

    # Combine information
    na_bonus_rows = get_weight(participation_rows)
    row_bonus = na_bonus_rows * percent_na + smooth_rep * (1 - percent_na)
    na_bonus_columns = get_weight(participation_columns)
    col_bonus = na_bonus_columns * percent_na + consensus_reward * (1 - percent_na)

    return(row_bonus, num_voters)
